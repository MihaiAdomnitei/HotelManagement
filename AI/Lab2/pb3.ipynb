{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 3.Se da un fisier care contine un text (format din mai multe propozitii) in limba romana - a se vedea fisierul ”data/texts.txt”. Se cere sa se determine si sa se vizualizeze:\n",
    "#\n",
    "# numarul de propozitii din text;\n",
    "# numarul de cuvinte din text\n",
    "# numarul de cuvinte diferite din text\n",
    "# cel mai scurt si cel mai lung cuvant (cuvinte)\n",
    "# textul fara diacritice\n",
    "# sinonimele celui mai lung cuvant din text"
   ],
   "id": "de187f4c79f30f72"
  },
  {
   "cell_type": "code",
   "id": "40a03dc6-691e-4c78-8c44-75727d395e5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-11T13:34:30.881962Z",
     "start_time": "2025-03-11T13:34:30.567979Z"
    }
   },
   "source": [
    "import unidecode\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_synonyms(word):\n",
    "    url = f\"https://dexonline.ro/intrare/{word}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: Could not access DEX Online for '{word}'.\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    synonyms = set()\n",
    "    for span in soup.find_all(\"span\", class_=\"badge-relation badge-relation-1\"):\n",
    "        link = span.find(\"a\")\n",
    "        if link:\n",
    "            synonyms.add(link.text.strip())\n",
    "\n",
    "    return list(synonyms)\n",
    "\n",
    "def normalize_word(word):\n",
    "    cleaned_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "    normalized_word = re.sub(r'(.)\\1+', r'\\1', cleaned_word)\n",
    "    return normalized_word\n",
    "\n",
    "def analyze_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    sentences = text.split('.')\n",
    "    num_sentences = len([s for s in sentences if s.strip()])\n",
    "\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "\n",
    "    unique_words = set(words)\n",
    "    num_unique_words = len(unique_words)\n",
    "\n",
    "    sorted_words = sorted(words, key=len)\n",
    "    shortest_word = sorted_words[0]\n",
    "    longest_word = sorted_words[-1]\n",
    "\n",
    "    text_without_diacritics = unidecode.unidecode(text)\n",
    "\n",
    "    # Normalize the longest word to remove repeated characters\n",
    "    normalized_longest_word = normalize_word(longest_word)\n",
    "\n",
    "    synonyms = get_synonyms(normalized_longest_word)\n",
    "\n",
    "    print(\"Number of sentences:\", num_sentences)\n",
    "    print(\"Number of words:\", num_words)\n",
    "    print(\"Number of unique words:\", num_unique_words)\n",
    "    print(\"Shortest word:\", shortest_word)\n",
    "    print(\"Longest word:\", longest_word)\n",
    "    print(\"Text without diacritics:\", text_without_diacritics)\n",
    "    print(\"Synonyms of the longest word:\", synonyms)\n",
    "\n",
    "analyze_text('data/texts.txt')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 9\n",
      "Number of words: 161\n",
      "Number of unique words: 105\n",
      "Shortest word: o\n",
      "Longest word: ”Confiiiiiiiiiiiiiiiiiiiiiiiiiiiirm\n",
      "Text without diacritics: Mesaj de informare: \n",
      "Cursul si laboratoarele de Inteligenta Artificiala vor fi o \n",
      "provocare pentru toti. Suntem convinsi ca veti realiza proiecte \n",
      "foarte interesante. Va incurajam sa adresati intrebari atunci \n",
      "cand ceva nu e clar, atat in mod live, cat si folosind platforma \n",
      "Teams, canalul \"general\". \n",
      "Daca ati citit pana aici, va rugam sa lasati un mesaj pe canalul \n",
      "general cu textul \"Confiiiiiiiiiiiiiiiiiiiiiiiiiiiirm ca am citit \n",
      "textul pentru problema 3 din lab2\". \n",
      "--\n",
      "Mesaj de informare generat de ChatGPT:\n",
      "Stimati cursanti,\n",
      "Suntem incantati sa va avem in echipa noastra pentru Cursul si \n",
      "laboratoarele de Inteligenta Artificiala. Aceasta experienta va \n",
      "fi o adevarata provocare, dar suntem convinsi ca veti realiza \n",
      "proiecte extrem de interesante.\n",
      "Va incurajam sa fiti activi si sa adresati intrebari atunci cand \n",
      "ceva nu este clar. Fie ca este vorba de o discutie in timp real \n",
      "sau prin intermediul platformei Teams, canalul \"general\", suntem \n",
      "aici sa va sprijinim.\n",
      "Succes si sa inceapa aventura AI!\n",
      "Cu consideratie, Echipa de Inteligenta Artificiala\n",
      "Synonyms of the longest word: ['definitiva', 'corobora', 'adeveri']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "12c1da30b121e221"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
